{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code script is to sample and generate data for representation learning on Amazon networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import itertools\n",
    "import functools\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Set, Tuple\n",
    "import numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_root:str, dataset:str, sub_dataset:str) -> Dict:\n",
    "    \"\"\"\n",
    "    data_root: path to directory contains the data file.\n",
    "    dataset: path to dataset (Amazon)\n",
    "    subdataset: sub dataset name (e.g. sports)\n",
    "\n",
    "    Returns:\n",
    "    data: Dict, key is the doc id, and value is data entry\n",
    "    \"\"\"\n",
    "    # read raw data\n",
    "    data_path = os.path.join(data_root, dataset, sub_dataset, 'product.json')\n",
    "    brand_dict = defaultdict(set)\n",
    "    with open(data_path) as f:\n",
    "        data = {}\n",
    "        readin = f.readlines()\n",
    "        for line in tqdm(readin, desc=\"Loading Data...\"):\n",
    "            tmp = eval(line.strip())\n",
    "            k = tmp['asin']\n",
    "            data[k] = tmp\n",
    "            if 'brand' in tmp:\n",
    "                brand_dict[tmp['brand']].add(k)\n",
    "    for k in data:\n",
    "        if 'related' not in data[k]:\n",
    "            data[k]['related'] = {}\n",
    "        if 'brand' in data[k]:\n",
    "            data[k]['related']['cobrand'] = brand_dict[data[k]['brand']]\n",
    "        else:\n",
    "            data[k]['related']['cobrand'] = set()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_dump(data: Dict, tuples: Set[Tuple[str, str]], path: str) -> None:\n",
    "    \"\"\"\n",
    "    Dump the sampled pairs into jsonl file\n",
    "\n",
    "    data: Dataset returned by `load_data`\n",
    "    tuples: Sampled tuples\n",
    "    path: path to save json file\n",
    "    \"\"\"\n",
    "    print(\"Dump data to %s\" % path)\n",
    "    cnt = 0\n",
    "    with open(path, 'w') as fout:\n",
    "        for t in tqdm(tuples, desc=\"Processing %s\" % path.split('/')[-1]):\n",
    "            q, k = t\n",
    "            if q in data and k in data and 'title' in data[q] and 'title' in data[k] and data[q]['title'].strip() != '' and data[k]['title'].strip() != '':\n",
    "                cur = {}\n",
    "                cur['q_text'] = data[q]['title']\n",
    "                if 'description' in data[q]:\n",
    "                    cur['q_text'] += ' ' + data[q]['description']\n",
    "                cur['k_text'] = data[k]['title']\n",
    "                if 'description' in data[k]:\n",
    "                    cur['k_text'] += ' ' + data[k]['description']\n",
    "                fout.write(json.dumps(cur)+'\\n')\n",
    "                cnt += 1\n",
    "    print('%d entries written' % cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_no_intermediate(data: Dict, type: List[str])-> Set[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Build relationship by type, no intermediate node\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    for k0 in tqdm(data):\n",
    "        if 'related' in data[k0]:\n",
    "            if type[0] in data[k0]['related']:\n",
    "                tmp = data[k0]['related'][type[0]]\n",
    "                for k1 in tmp:\n",
    "                    if k1 != k0:\n",
    "                        if k1 < k0:\n",
    "                            k0, k1 = k1, k0\n",
    "                            pairs.add((k0, k1))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_DICT = {\n",
    "    'also_viewed': build_no_intermediate,\n",
    "    'also_bought': build_no_intermediate,\n",
    "    'bought_together': build_no_intermediate,\n",
    "    'cobrand': build_no_intermediate,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = 'amazon'\n",
    "sub_datasets = ['cloth', 'home', 'sports'][2]\n",
    "base_dir = 'xxx/data/'\n",
    "save_dir = f'xxx/data/{sub_datasets}/raw'\n",
    "\n",
    "#save_dir = os.path.join(save_dir_base, datasets, sub_datasets)\n",
    "print(save_dir)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_d = load_data(base_dir, datasets, sub_datasets)\n",
    "print(len(cur_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, k in enumerate(cur_d):\n",
    "    if i == 5: break\n",
    "    print(k)\n",
    "    print(cur_d[k])\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "also_viewed = GENERATOR_DICT['also_viewed'](cur_d, ['also_viewed'])\n",
    "convert_and_dump(cur_d, also_viewed, os.path.join(save_dir, 'also_viewed.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "also_bought = GENERATOR_DICT['also_bought'](cur_d, ['also_bought'])\n",
    "convert_and_dump(cur_d, also_bought, os.path.join(save_dir, 'also_bought.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bought_together = GENERATOR_DICT['bought_together'](cur_d, ['bought_together'])\n",
    "convert_and_dump(cur_d, bought_together, os.path.join(save_dir, 'bought_together.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cobrand = GENERATOR_DICT['cobrand'](cur_d, ['cobrand'])\n",
    "convert_and_dump(cur_d, cobrand, os.path.join(save_dir, 'cobrand.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "684dd79593b7a31fd1a3d9b9f79e274b2833c74c2798fa00dbf737909b6a5be6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
