{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import itertools\n",
    "import functools\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Set, Tuple\n",
    "import numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_root:str, dataset:str, sub_dataset:str) -> Dict:\n",
    "    \"\"\"\n",
    "    data_root: path to directory contains the data file.\n",
    "    dataset: path to dataset (MAG)\n",
    "    subdataset: sub dataset name (e.g. CS)\n",
    "\n",
    "    Returns:\n",
    "    data: Dict, key is the doc id, and value is data entry\n",
    "    \"\"\"\n",
    "    # read raw data\n",
    "    data_path = os.path.join(data_root, dataset, sub_dataset, 'papers_bert.json')\n",
    "    with open(data_path) as f:\n",
    "        data = {}\n",
    "        readin = f.readlines()\n",
    "        for line in tqdm(readin, desc=\"Loading Data...\"):\n",
    "            tmp = eval(line.strip())\n",
    "            k = tmp['paper']\n",
    "            data[k] = tmp\n",
    "            data[k]['citation'] = []\n",
    "    for k in data:\n",
    "        refs = data[k]['reference']\n",
    "        new_refs = []\n",
    "        for paper in refs:\n",
    "            if paper in data:\n",
    "                new_refs.append(paper)\n",
    "                data[paper]['citation'].append(k)\n",
    "        data[k]['reference'] = new_refs\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meta_data(base_dir, data_name):\n",
    "    p = os.path.join(base_dir, data_name, 'venues.txt')\n",
    "    vid2name = defaultdict(str)\n",
    "    with open(p) as f:\n",
    "        for l in f:\n",
    "            id, _, name = l.strip().split('\\t')\n",
    "            vid2name[id] = name\n",
    "    return vid2name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label(data_path, subdataset):\n",
    "    path = os.path.join(data_path, 'MAG', subdataset, 'labels.txt')\n",
    "    id2label = dict()\n",
    "    id2level = dict()\n",
    "    with open(path) as f:\n",
    "        for l in tqdm(f):\n",
    "            if len(l.strip()) > 0:\n",
    "                lid, name, level = l.strip().split('\\t')\n",
    "                id2label[lid] = name\n",
    "                id2level[lid] = int(level)\n",
    "    return id2label, id2level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_base_dir = 'xxx/'\n",
    "raw_base_dir = 'xxx/data/'\n",
    "save_base_dir = 'xxx/data/'\n",
    "data_name = ['Geology', 'Mathematics'][1]\n",
    "meta_data_dir = os.path.join(meta_base_dir, data_name)\n",
    "raw_data_dir = os.path.join(raw_base_dir, data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(raw_base_dir, 'MAG', data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid2n = load_meta_data(meta_base_dir, data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label,id2level = load_label(raw_base_dir, data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paper recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_user_action_data(base_dir, subdataset):\n",
    "    p = os.path.join(base_dir, subdataset, 'raw', 'PaperRecommendation.txt')\n",
    "    ret = []\n",
    "    with open(p) as f:\n",
    "        for l in tqdm(f, desc='Loading user action data of %s...' % subdataset):\n",
    "            res = l.strip().split('\\t')\n",
    "            ret.append(res)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paper_recommendation(paper_data, user_data, threshold):\n",
    "    res = []\n",
    "    for item in tqdm(user_data, desc='Generating paper recommendation data'):\n",
    "        if len(item) == 3:\n",
    "            pa, pb, score = item\n",
    "            if 'title' in paper_data[pa] and 'title' in paper_data[pb]:\n",
    "                ta = paper_data[pa]['title'].strip()\n",
    "                tb = paper_data[pb]['title'].strip()\n",
    "                if len(ta) > 0 and len(tb) > 0 and float(score) > threshold:\n",
    "                    res.append((ta, tb, score))\n",
    "    return res        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_paper_recommendation(save_base_dir, data_name, data):\n",
    "    tmp_base = os.path.join(save_base_dir, data_name, 'downstream', 'paper_recommendation')\n",
    "    if not os.path.exists(tmp_base):\n",
    "        os.makedirs(tmp_base)\n",
    "    data_path = os.path.join(tmp_base, 'data.jsonl')\n",
    "    with open(data_path, 'w') as f:\n",
    "        for item in tqdm(data, desc=\"Write data to %s\" % data_path):\n",
    "            f.write(\n",
    "                json.dumps({\n",
    "                    'q_text': item[0],\n",
    "                    'k_text': item[1],\n",
    "                    'score': float(item[2])\n",
    "                }) + '\\n'\n",
    "            )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_action_data = load_raw_user_action_data(save_base_dir, data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(user_action_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = generate_paper_recommendation(data, user_action_data, 0.9)\n",
    "print(len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_paper_recommendation(save_base_dir, data_name, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## author disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_author_disambiguation(data):\n",
    "    dict_author_id = defaultdict(list)\n",
    "    res = set()\n",
    "    res3 = defaultdict(dict)\n",
    "    for k in tqdm(data):\n",
    "        if 'author' in data[k]:\n",
    "            for tmp_a in data[k]['author']:\n",
    "                res.add((data[k]['title'], tmp_a))\n",
    "                dict_author_id[tmp_a].append(data[k]['title'])\n",
    "    print(len(dict_author_id))\n",
    "    for k in dict_author_id:\n",
    "        if len(dict_author_id[k]) > 100:\n",
    "            tmp = tuple(random.sample(dict_author_id[k], 100))\n",
    "        else:\n",
    "            tmp = tuple(dict_author_id[k])\n",
    "        res3[k] = {'id': k, 'paper': tmp}\n",
    "    print(len(res), len(res3))\n",
    "    return res, res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_author_advanced(save_base_dir, data_name, data, author_info_dict):\n",
    "    tmp_base = os.path.join(save_base_dir, data_name, 'downstream', 'author_disambiguation')\n",
    "    if not os.path.exists(tmp_base):\n",
    "        os.makedirs(tmp_base)\n",
    "    \n",
    "    # author paper matching\n",
    "    data_ap_path = os.path.join(tmp_base, 'data_ap.jsonl')\n",
    "    with open(data_ap_path, 'w') as f:\n",
    "        for pname, aid in tqdm(data):\n",
    "            tmp_paper = list(author_info_dict[aid]['paper'])\n",
    "            if len(tmp_paper) == 1:\n",
    "                continue\n",
    "            if pname in tmp_paper:\n",
    "                tmp_paper.remove(pname)\n",
    "            random.shuffle(tmp_paper)\n",
    "            dd = {'q_text': pname, 'k_text': ' '.join(tmp_paper)}\n",
    "            ddr = json.dumps(dd)\n",
    "            f.write(ddr + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d, vinfo = generate_author_disambiguation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_author_advanced(save_base_dir, data_name, d, vinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## venue recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_venue_recommendation(data, vid2n):\n",
    "    dict_venue_id = defaultdict(list)\n",
    "    res = set()\n",
    "    res2 = []\n",
    "    res3 = defaultdict(dict)\n",
    "    for k in tqdm(data):\n",
    "        if 'venue' in data[k]:\n",
    "            res.add((data[k]['title'], data[k]['venue']))\n",
    "            dict_venue_id[data[k]['venue']].append(data[k]['title'])\n",
    "    print(len(dict_venue_id))\n",
    "    for k in dict_venue_id:\n",
    "        n = vid2n[k]\n",
    "        if len(dict_venue_id[k]) > 100:\n",
    "            tmp = tuple(random.sample(dict_venue_id[k], 100))\n",
    "        else:\n",
    "            tmp = tuple(dict_venue_id[k])\n",
    "        res2.append({'id': k, 'name': n, 'paper': tmp})\n",
    "        res3[k] = {'id': k, 'name': n, 'paper': tmp}\n",
    "    print(len(res), len(res2))\n",
    "    return res, res2, res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_venue(save_base_dir, data_name, data, venue_info):\n",
    "    tmp_base = os.path.join(save_base_dir, data_name, 'downstream', 'venue_recommendation')\n",
    "    if not os.path.exists(tmp_base):\n",
    "        os.makedirs(tmp_base)\n",
    "    data_path = os.path.join(tmp_base, 'data.tsv')\n",
    "    meta_data_path = os.path.join(tmp_base, 'venue.jsonl')\n",
    "    with open(data_path, 'w') as f:\n",
    "        for pname, vid in data:\n",
    "            f.write(\"%s\\t%s\\n\" % (pname, str(vid)))\n",
    "    with open(meta_data_path, 'w') as f:\n",
    "        for md in venue_info:\n",
    "            f.write(json.dumps(md)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_venue_advanced(save_base_dir, data_name, data, venue_info_dict):\n",
    "    tmp_base = os.path.join(save_base_dir, data_name, 'downstream', 'venue_recommendation')\n",
    "    if not os.path.exists(tmp_base):\n",
    "        os.makedirs(tmp_base)\n",
    "    \n",
    "    # venue name matching\n",
    "    data_vn_path = os.path.join(tmp_base, 'data_vn.jsonl')\n",
    "    with open(data_vn_path, 'w') as f:\n",
    "        for pname, vid in tqdm(data):\n",
    "            dd = {'q_text': pname, 'k_text': venue_info_dict[vid]['name']}\n",
    "            ddr = json.dumps(dd)\n",
    "            f.write(ddr + '\\n')\n",
    "    \n",
    "    \n",
    "    # venue paper matching\n",
    "    data_vp_path = os.path.join(tmp_base, 'data_vp.jsonl')\n",
    "    with open(data_vp_path, 'w') as f:\n",
    "        for pname, vid in tqdm(data):\n",
    "            tmp_paper = list(venue_info_dict[vid]['paper'])\n",
    "            random.shuffle(tmp_paper)\n",
    "            dd = {'q_text': pname, 'k_text': ' '.join(tmp_paper)}\n",
    "            ddr = json.dumps(dd)\n",
    "            f.write(ddr + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, v, vinfo = generate_venue_recommendation(data, vid2n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_venue(save_base_dir, data_name, d, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write_venue_advanced(save_base_dir, data_name, d, vinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_regression(data, kw_name, func=None):\n",
    "    res = set()\n",
    "    for k in data:\n",
    "        if kw_name in data[k]:\n",
    "            v = data[k][kw_name]\n",
    "            t = data[k]['title']\n",
    "            if func is not None:\n",
    "                v = func(v)\n",
    "            res.add((t, v))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_regression(save_base_dir, data_name, task_name, data, theshold, minus=0):\n",
    "    tmp_base = os.path.join(save_base_dir, data_name, 'downstream', task_name)\n",
    "    if not os.path.exists(tmp_base):\n",
    "        os.makedirs(tmp_base)\n",
    "    data_path = os.path.join(tmp_base, 'data.jsonl')\n",
    "    print(\"Write to %s\" % data_path)\n",
    "    with open(data_path, 'w') as f:\n",
    "        for p, v in tqdm(data):\n",
    "            if float(v) > theshold:\n",
    "                continue\n",
    "            f.write(json.dumps({\"q_text\":p, \"label\":float(v)-minus})+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cite_pred = generate_regression(data, 'citation', len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_pred = generate_regression(data, 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## statistics of citation\n",
    "citation_cnt_list = []\n",
    "for tmp in cite_pred:\n",
    "    citation_cnt_list.append(tmp[1])\n",
    "sns.kdeplot(citation_cnt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## statistics of citation\n",
    "year_list = []\n",
    "for tmp in year_pred:\n",
    "    year_list.append(int(tmp[1]))\n",
    "print(np.min([float(y) for y in year_list]))\n",
    "sns.kdeplot(year_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_regression(save_base_dir, data_name, 'citation_prediction', cite_pred, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !!! change the down minus threshold\n",
    "write_regression(save_base_dir, data_name, 'year_prediction', year_pred, 20000, 1981.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification(data, id2level, id2label):\n",
    "    res = set()\n",
    "    coarse_labels = []\n",
    "    coarse_labels_dict = dict()\n",
    "    coarse_labels_cnt = defaultdict(int)\n",
    "    for id, lv in id2level.items():\n",
    "        if lv == 1:\n",
    "            coarse_labels.append(id)\n",
    "    coarse_labels = sorted(coarse_labels)\n",
    "    for idx_for_classification, lid in enumerate(coarse_labels):\n",
    "        coarse_labels_dict[lid] = idx_for_classification\n",
    "    print(len(coarse_labels_dict), coarse_labels_dict)\n",
    "    for k in tqdm(data):\n",
    "        if 'label' in data[k] and 'title' in data[k]:\n",
    "            c_labels = data[k]['label'] \n",
    "            for lid in c_labels:\n",
    "                lid = str(lid)\n",
    "                if lid in id2level and lid in coarse_labels_dict:\n",
    "                    res.add((data[k]['title'], coarse_labels_dict[lid]))\n",
    "                    coarse_labels_cnt[lid] += 1\n",
    "    print(\"Number of label %d\" % len(coarse_labels_cnt))\n",
    "    print(\"Label Mapping\")\n",
    "    print({id2label[x]: coarse_labels_dict[x] for x in coarse_labels_dict})\n",
    "    print(\"Label Count\")\n",
    "    print({id2label[x]: coarse_labels_cnt[x] for x in coarse_labels_cnt})\n",
    "    # sns.barplot(data=list(coarse_labels_cnt.values()))\n",
    "    return res, {id2label[x]: coarse_labels_dict[x] for x in coarse_labels_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(save_base_dir, data_name, task_name, data):\n",
    "    tmp_base = os.path.join(save_base_dir, data_name, 'downstream', task_name)\n",
    "    data_path = os.path.join(tmp_base, 'label.jsonl') \n",
    "    with open(data_path,'w') as fout:\n",
    "        json.dump(data, fout, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_classification(save_base_dir, data_name, task_name, data):\n",
    "    tmp_base = os.path.join(save_base_dir, data_name, 'downstream', task_name)\n",
    "    if not os.path.exists(tmp_base):\n",
    "        os.makedirs(tmp_base)\n",
    "    data_path = os.path.join(tmp_base, 'data.jsonl')\n",
    "    print(\"Write to %s\" % data_path)\n",
    "    with open(data_path, 'w') as f:\n",
    "        for p, v in tqdm(data):\n",
    "            #f.write('%s\\t%s\\n' % (p, str(v)))\n",
    "            f.write(json.dumps({\"q_text\":p, \"label\":v})+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coarse_classification, id2label = generate_classification(data, id2level, id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_classification(save_base_dir, data_name, 'coarse_classification', coarse_classification)\n",
    "write_json(save_base_dir, data_name, 'coarse_classification', id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_retrieval_label(label_dir):\n",
    "    # read label name dict\n",
    "    label_name_dict = {}\n",
    "    label_name_set = set()\n",
    "    label_name2id_dict = {}\n",
    "\n",
    "    with open(label_dir) as f:\n",
    "        readin = f.readlines()\n",
    "        for line in tqdm(readin):\n",
    "            tmp = line.strip().split('\\t')\n",
    "            label_name_dict[tmp[0]] = tmp[1]\n",
    "            label_name2id_dict[tmp[1]] = tmp[0]\n",
    "            label_name_set.add(tmp[1])\n",
    "\n",
    "    print(f'Num of unique labels:{len(label_name_set)}')\n",
    "    \n",
    "    return label_name_dict, label_name2id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_retrieval_base(save_base_dir, data_name, data, label_name_dict, label_name2id_dict):\n",
    "    # save node labels\n",
    "    random.seed(0)\n",
    "\n",
    "    tmp_base = os.path.join(save_base_dir, data_name, 'downstream', 'retrieval')\n",
    "    if not os.path.exists(tmp_base):\n",
    "        os.makedirs(tmp_base)\n",
    "    data_path = os.path.join(tmp_base, 'node_classification.jsonl')\n",
    "    \n",
    "    with open(data_path, 'w') as fout:\n",
    "        for q in tqdm(data):\n",
    "\n",
    "            q_text = data[q]['title']\n",
    "\n",
    "            label_names_list = list(set([label_name_dict[lid] for lid in data[q]['label']]))\n",
    "            label_ids_list = [label_name2id_dict[lname] for lname in label_names_list]\n",
    "\n",
    "            fout.write(json.dumps({\n",
    "                'q_text':q_text,\n",
    "                'labels':label_ids_list,\n",
    "                'label_names':label_names_list\n",
    "            })+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_retrieval(save_base_dir, data_name, data, label_name2id_dict):\n",
    "    tmp_base = os.path.join(save_base_dir, data_name, 'downstream', 'retrieval')\n",
    "    if not os.path.exists(tmp_base):\n",
    "        os.makedirs(tmp_base)\n",
    "    \n",
    "    label_json_path = os.path.join(tmp_base, 'documents.json')\n",
    "    print(\"Write to %s\" % label_json_path)\n",
    "    labels_dict = []\n",
    "    for lname in label_name2id_dict:\n",
    "        if lname != 'null':\n",
    "            labels_dict.append({'id':label_name2id_dict[lname], 'contents':lname})\n",
    "    json.dump(labels_dict, open(label_json_path, 'w'), indent=4)\n",
    "\n",
    "    label_txt_path = os.path.join(tmp_base, 'documents.txt')\n",
    "    print(\"Write to %s\" % label_txt_path)\n",
    "    with open(label_txt_path, 'w') as fout:\n",
    "        for lname in label_name2id_dict:\n",
    "            if lname == 'null':\n",
    "                continue\n",
    "            fout.write(label_name2id_dict[lname]+'\\t'+lname+'\\n')\n",
    "            \n",
    "    docid = 0\n",
    "    data_class_path = os.path.join(tmp_base, 'node_classification.jsonl')\n",
    "    print(\"Read from %s\" % data_class_path)\n",
    "    node_text_path = os.path.join(tmp_base, 'node_text.tsv')\n",
    "    print(\"Write to %s\" % node_text_path)\n",
    "    trec_path = os.path.join(tmp_base, 'truth.trec')\n",
    "    print(\"Write to %s\" % trec_path)\n",
    "    with open(data_class_path) as f, open(node_text_path, 'w') as fout1, open(trec_path, 'w') as fout2:\n",
    "        readin = f.readlines()\n",
    "        for line in tqdm(readin):\n",
    "            tmp = json.loads(line)\n",
    "            fout1.write(str(docid) + '\\t' + tmp['q_text'] + '\\n')\n",
    "            for label in tmp['labels']:\n",
    "                fout2.write(str(docid)+' '+str(0)+' '+label+' '+str(1)+'\\n')\n",
    "            docid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dir = os.path.join(raw_base_dir, 'MAG', data_name, 'labels.txt')\n",
    "label_name_dict, label_name2id_dict = init_retrieval_label(label_dir)\n",
    "write_retrieval_base(save_base_dir, data_name, data, label_name_dict, label_name2id_dict)\n",
    "write_retrieval(save_base_dir, data_name, data, label_name2id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "684dd79593b7a31fd1a3d9b9f79e274b2833c74c2798fa00dbf737909b6a5be6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
