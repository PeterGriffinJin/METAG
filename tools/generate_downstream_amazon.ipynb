{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import itertools\n",
    "import functools\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Set, Tuple\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_root:str, dataset:str, sub_dataset:str) -> Dict:\n",
    "    \"\"\"\n",
    "    data_root: path to directory contains the data file.\n",
    "    dataset: path to dataset (Amazon)\n",
    "    subdataset: sub dataset name (e.g. sports)\n",
    "\n",
    "    Returns:\n",
    "    data: Dict, key is the doc id, and value is data entry\n",
    "    \"\"\"\n",
    "    # read raw data\n",
    "    data_path = os.path.join(data_root, dataset, sub_dataset, 'product.json')\n",
    "    brand_dict = defaultdict(set)\n",
    "    with open(data_path) as f:\n",
    "        data = {}\n",
    "        readin = f.readlines()\n",
    "        for line in tqdm(readin, desc=\"Loading Data...\"):\n",
    "            tmp = eval(line.strip())\n",
    "            if ('title' in tmp and len(tmp['title'].strip()) != 0) or ('description' in tmp and len(tmp['description'].strip()) != 0):\n",
    "                k = tmp['asin']\n",
    "                data[k] = tmp\n",
    "                if 'brand' in tmp:\n",
    "                    brand_dict[tmp['brand']].add(k)\n",
    "    for k in data:\n",
    "        if 'related' not in data[k]:\n",
    "            data[k]['related'] = {}\n",
    "        if 'brand' in data[k]:\n",
    "            data[k]['related']['cobrand'] = brand_dict[data[k]['brand']]\n",
    "        else:\n",
    "            data[k]['related']['cobrand'] = set()\n",
    "    print(len(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_new_data(data_root, sub_dataset):\n",
    "    data_path = os.path.join(data_root, sub_dataset, 'product.json')\n",
    "    with open(data_path) as f:\n",
    "        data = {}\n",
    "        readin = f.readlines()\n",
    "        for line in tqdm(readin, desc=\"Loading Data...\"):\n",
    "            tmp = eval(line.strip())\n",
    "            k = tmp['asin']\n",
    "            data[k] = tmp\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_oracle(data_root, sub_dataset):\n",
    "    label_oracle_path = os.path.join(data_root, 'amazon', sub_dataset, 'coarse_class.txt')\n",
    "    label_oracle = set()\n",
    "    with open(label_oracle_path) as f:\n",
    "        for l in f:\n",
    "            _, l, cnt = l.split('\\t')\n",
    "            label_oracle.add(l)\n",
    "    return label_oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_both(old_base_dir, new_base_dir, dataset, subdataset):\n",
    "    old_data = load_data(old_base_dir, dataset, subdataset)\n",
    "    new_data = load_new_data(new_base_dir, subdataset)\n",
    "    return old_data, new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_feature(old, new):\n",
    "    for k in tqdm(old):\n",
    "        if k in new and 'feature' in new[k] and len(new[k]['feature']) > 0:\n",
    "            old[k]['feature'] = new[k]['feature']\n",
    "    return old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    p_text = ' '.join(text.split('\\r\\n'))\n",
    "    p_text = ' '.join(text.split('\\n\\r'))\n",
    "    p_text = ' '.join(text.split('\\n'))\n",
    "    p_text = ' '.join(p_text.split('\\t'))\n",
    "    p_text = ' '.join(p_text.split('\\rm'))\n",
    "    p_text = ' '.join(p_text.split('\\r'))\n",
    "    p_text = ''.join(p_text.split('$'))\n",
    "    p_text = ''.join(p_text.split('*'))\n",
    "\n",
    "    return p_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_description(data, k):\n",
    "    tt = ''\n",
    "    if 'title' in data[k]:\n",
    "        tt = text_process(data[k]['title'])\n",
    "    if 'description' in data[k]:\n",
    "        tt += ' ' + text_process(data[k]['description'])\n",
    "    return tt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = 'amazon'\n",
    "\n",
    "data_name = ['cloth', 'sports', 'home'][2]\n",
    "old_raw_base_dir = 'xxx/data/'\n",
    "new_raw_base_dir = 'xxx/data/'\n",
    "save_base_dir = 'xxx/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = load_both(old_raw_base_dir, new_raw_base_dir, 'amazon', data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = link_feature(data[0], data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_oracle = load_label_oracle(old_raw_base_dir, data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regression task (price prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_regression(data, kw):\n",
    "    ret = set()\n",
    "    for k in tqdm(data, desc=\"Generate %s\" % kw):\n",
    "        if kw in data[k]:\n",
    "            tt = get_title_description(data, k)\n",
    "            if tt is not None:\n",
    "                ret.add((tt, data[k][kw]))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_regression(save_base_dir, data_name, task_name, data, theshold):\n",
    "    tmp_base = os.path.join(save_base_dir, data_name, 'downstream', task_name)\n",
    "    if not os.path.exists(tmp_base):\n",
    "        os.makedirs(tmp_base)\n",
    "    data_path = os.path.join(tmp_base, 'data.jsonl')\n",
    "    print(\"Write to %s\" % data_path)\n",
    "    with open(data_path, 'w') as f:\n",
    "        for p, v in tqdm(data, desc=\"Write %s\" % task_name):\n",
    "            #f.write('%s\\t%s\\n' % (p, str(v)))\n",
    "            if v > theshold:\n",
    "                continue\n",
    "            f.write(json.dumps({\"q_text\":p, \"label\":v})+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate price prediction\n",
    "res = generate_regression(data, 'price')\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## statistics of price\n",
    "prices_list = []\n",
    "for tmp in res:\n",
    "    prices_list.append(tmp[1])\n",
    "sns.kdeplot(prices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### !! select the price theshold based on the density function above\n",
    "\n",
    "theshold=100\n",
    "write_regression(save_base_dir, data_name, 'price', res, theshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## brand prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_brand(data):\n",
    "    brand_dict = defaultdict(set)\n",
    "    brand_json = []\n",
    "    brand_json_dict = defaultdict(dict)\n",
    "    brand_data_tsv = []\n",
    "    for k in tqdm(data, desc=\"Generate Brand Data\"):\n",
    "        if 'brand' in data[k] and data[k]['brand'] != 'Unknown':\n",
    "            brand_dict[data[k]['brand']].add(k)\n",
    "    for b in tqdm(brand_dict, desc=\"Generate Brand Dict\"):\n",
    "        for iid in brand_dict[b]:\n",
    "            brand_data_tsv.append((get_title_description(data, iid), b))\n",
    "        if len(brand_dict[b]) >= 100:\n",
    "            sampled = random.sample(list(brand_dict[b]), 100)\n",
    "        else:\n",
    "            sampled = list(brand_dict[b])\n",
    "        cur = {'name': b, 'items': [get_title_description(data, x) for x in sampled]}\n",
    "        brand_json.append(cur)\n",
    "        brand_json_dict[b] = cur\n",
    "    return brand_data_tsv, brand_json, brand_json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_brand(save_base_dir, data_name, data, brand_info):\n",
    "    tmp_base = os.path.join(save_base_dir, data_name, 'downstream', 'brand')\n",
    "    if not os.path.exists(tmp_base):\n",
    "        os.makedirs(tmp_base)\n",
    "    data_path = os.path.join(tmp_base, 'data.tsv')\n",
    "    meta_data_path = os.path.join(tmp_base, 'brand.jsonl')\n",
    "    print(\"Write to %s\" % data_path)\n",
    "    print(\"Write to %s\" % meta_data_path)\n",
    "    with open(data_path, 'w') as f:\n",
    "        for pname, vid in data:\n",
    "            f.write(\"%s\\t%s\\n\" % (pname, str(vid)))\n",
    "    with open(meta_data_path, 'w') as f:\n",
    "        for md in brand_info:\n",
    "            f.write(json.dumps(md)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_brand_advanced(save_base_dir, data_name, data, brand_info_dict):\n",
    "    tmp_base = os.path.join(save_base_dir, data_name, 'downstream', 'brand')\n",
    "    if not os.path.exists(tmp_base):\n",
    "        os.makedirs(tmp_base)\n",
    "    \n",
    "    # brand name matching\n",
    "    data_vn_path = os.path.join(tmp_base, 'data_bn.jsonl')\n",
    "    with open(data_vn_path, 'w') as f:\n",
    "        for pname, vid in tqdm(data):\n",
    "            dd = {'q_text': pname, 'k_text': brand_info_dict[vid]['name']}\n",
    "            ddr = json.dumps(dd)\n",
    "            f.write(ddr + '\\n')\n",
    "    \n",
    "    # brand item matching\n",
    "    data_vp_path = os.path.join(tmp_base, 'data_bi.jsonl')\n",
    "    with open(data_vp_path, 'w') as f:\n",
    "        for pname, vid in tqdm(data):\n",
    "            tmp_items = list(brand_info_dict[vid]['items'])\n",
    "            if len(tmp_items) == 1:\n",
    "                continue\n",
    "            if pname in tmp_items:\n",
    "                tmp_items.remove(pname)\n",
    "            random.shuffle(tmp_items)\n",
    "            dd = {'q_text': pname, 'k_text': ' '.join(tmp_items)}\n",
    "            ddr = json.dumps(dd)\n",
    "            f.write(ddr + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_brand = generate_brand(data)\n",
    "print(len(res_brand[0]), len(res_brand[1]))\n",
    "write_brand(save_base_dir, data_name, res_brand[0], res_brand[1])\n",
    "write_brand_advanced(save_base_dir, data_name, res_brand[0], res_brand[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature(data):\n",
    "    ret = set()\n",
    "    cnt = 0\n",
    "    item_cnt = 0\n",
    "    for k in tqdm(data, desc=\"Generate Feature\"):\n",
    "        if 'feature' in data[k]:\n",
    "            item_cnt += 1\n",
    "            tt = get_title_description(data, k)\n",
    "            for c in data[k]['feature']:\n",
    "                if c[:len('<span class')] != '<span class' and len(c) != 0:\n",
    "                    ret.add((tt, text_process(c)))\n",
    "                    cnt += 1\n",
    "    print(f'feature/item = {cnt/item_cnt}')\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_feature(save_base_dir, data_name, task_name, data):\n",
    "    tmp_base = os.path.join(save_base_dir, data_name, 'downstream', task_name)\n",
    "    if not os.path.exists(tmp_base):\n",
    "        os.makedirs(tmp_base)\n",
    "    data_path = os.path.join(tmp_base, 'data.jsonl')\n",
    "    print(\"Write to %s\" % data_path)\n",
    "    with open(data_path, 'w') as f:\n",
    "        for tt, c in tqdm(data, desc=\"Write Feature\"):\n",
    "            dd = {'q_text': tt, 'k_text': c}\n",
    "            ddr = json.dumps(dd)\n",
    "            f.write(ddr + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_res = generate_feature(data)\n",
    "print(len(f_res))\n",
    "write_feature(save_base_dir, data_name, 'feature_pred', f_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification(data, label_oracle, threshold=10000):\n",
    "    label_cnt = defaultdict(int)\n",
    "    label2idx = dict()\n",
    "    res = set()\n",
    "    for k in tqdm(data):\n",
    "        if 'categories' in data[k] and len(data[k]['categories']) == 1 and len(data[k]['categories'][0]) > 1:\n",
    "            ln = data[k]['categories'][0][1]\n",
    "            label_cnt[ln] += 1\n",
    "    tmp_list = []\n",
    "    for k in label_cnt:\n",
    "        if label_cnt[k] >= threshold and k in label_oracle:\n",
    "            tmp_list.append(k)\n",
    "    label_lst = sorted(tmp_list)\n",
    "    for idx,l in enumerate(label_lst):\n",
    "        label2idx[l] = idx\n",
    "    for k in tqdm(data):\n",
    "        tt = get_title_description(data, k).strip()\n",
    "        if len(tt) > 0 and 'categories' in data[k] and len(data[k]['categories']) == 1 and len(data[k]['categories'][0]) > 1:\n",
    "            ln = data[k]['categories'][0][1]\n",
    "            if ln in label2idx:\n",
    "                res.add((tt, label2idx[ln]))\n",
    "    print(len(label2idx))\n",
    "    print(label2idx)\n",
    "    print({x: label_cnt[x] for x in label2idx})\n",
    "    return res, label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_classification(save_base_dir, data_name, task_name, data):\n",
    "    tmp_base = os.path.join(save_base_dir, data_name, 'downstream', task_name)\n",
    "    if not os.path.exists(tmp_base):\n",
    "        os.makedirs(tmp_base)\n",
    "    data_path = os.path.join(tmp_base, 'data.jsonl')\n",
    "    print(\"Write to %s\" % data_path)\n",
    "    with open(data_path, 'w') as f:\n",
    "        for p, v in tqdm(data, desc=\"Write %s\" % task_name):\n",
    "            #f.write('%s\\t%s\\n' % (p, str(v)))\n",
    "            f.write(json.dumps({\"q_text\":p, \"label\":v})+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(save_base_dir, data_name, task_name, data):\n",
    "    tmp_base = os.path.join(save_base_dir, data_name, 'downstream', task_name)\n",
    "    data_path = os.path.join(tmp_base, 'label.jsonl') \n",
    "    with open(data_path,'w') as fout:\n",
    "        json.dump(data, fout, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data, label2idx = generate_classification(data, label_oracle, 0)\n",
    "print(len(classification_data))\n",
    "write_classification(save_base_dir, data_name, 'coarse_classification', classification_data)\n",
    "write_json(save_base_dir, data_name, 'coarse_classification', label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_retrieval_label(data):\n",
    "    # statistics on label names\n",
    "    label_name_stat = defaultdict(int)\n",
    "\n",
    "    for did in tqdm(data):\n",
    "        sample = data[did]\n",
    "        c_list = list(set(sum(sample['categories'], [])))\n",
    "        for c in c_list:\n",
    "            label_name_stat[c] += 1\n",
    "            \n",
    "    # read label name dict\n",
    "    label_name_dict = {}\n",
    "    label_name_set = set()\n",
    "    label_name2id_dict = {}\n",
    "\n",
    "    for n in label_name_stat:\n",
    "        if label_name_stat[n] > int(0.5 * len(data)):\n",
    "            continue\n",
    "\n",
    "        label_name_dict[len(label_name_dict)] = n\n",
    "        label_name_set.add(n)\n",
    "        label_name2id_dict[n] = len(label_name_dict) - 1\n",
    "\n",
    "    print(f'Num of unique labels:{len(label_name_set)}')\n",
    "    \n",
    "    return label_name2id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_retrieval_base(save_base_dir, data_name, task_name, data, label_name2id_dict):\n",
    "    tmp_base = os.path.join(save_base_dir, data_name, 'downstream', task_name)\n",
    "    if not os.path.exists(tmp_base):\n",
    "        os.makedirs(tmp_base)\n",
    "\n",
    "    random.seed(0)\n",
    "    data_path = os.path.join(tmp_base, 'node_classification.jsonl')\n",
    "    with open(data_path,'w') as fout:\n",
    "        for q in tqdm(data):\n",
    "            q_text = get_title_description(data, q)\n",
    "\n",
    "            label_names_list = list(set(sum(data[q]['categories'], [])))\n",
    "            label_names_list = [n for n in label_names_list if n in label_name2id_dict]\n",
    "            label_ids_list = [label_name2id_dict[n] for n in label_names_list]\n",
    "\n",
    "            if len(label_ids_list) != 0:\n",
    "                fout.write(json.dumps({\n",
    "                    'q_text':q_text,\n",
    "                    'labels':label_ids_list,\n",
    "                    'label_names':label_names_list\n",
    "                })+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_retrieval(save_base_dir, data_name, task_name, label_name2id_dict):\n",
    "    \n",
    "    tmp_base = os.path.join(save_base_dir, data_name, 'downstream', task_name)\n",
    "    if not os.path.exists(tmp_base):\n",
    "        os.makedirs(tmp_base)\n",
    "    \n",
    "    label_json_path = os.path.join(tmp_base, 'documents.json')\n",
    "    print(\"Write to %s\" % label_json_path)\n",
    "    labels_dict = []\n",
    "    for lname in label_name2id_dict:\n",
    "        if lname != 'null':\n",
    "            labels_dict.append({'id':label_name2id_dict[lname], 'contents':lname})\n",
    "    json.dump(labels_dict, open(label_json_path, 'w'), indent=4)\n",
    "\n",
    "    label_path = os.path.join(tmp_base, 'documents.txt')\n",
    "    print(\"Write to %s\" % label_path)\n",
    "    with open(label_path, 'w') as fout:\n",
    "        for lname in label_name2id_dict:\n",
    "            if lname == 'null':\n",
    "                continue\n",
    "            fout.write(str(label_name2id_dict[lname])+'\\t'+lname+'\\n')\n",
    "    \n",
    "    docid = 0\n",
    "    data_class_path = os.path.join(tmp_base, 'node_classification.jsonl')\n",
    "    print(\"Read from %s\" % data_class_path)\n",
    "    node_text_path = os.path.join(tmp_base, 'node_text.tsv')\n",
    "    print(\"Write to %s\" % node_text_path)\n",
    "    trec_path = os.path.join(tmp_base, 'truth.trec')\n",
    "    print(\"Write to %s\" % trec_path)\n",
    "    with open(data_class_path) as f, open(node_text_path, 'w') as fout1, open(trec_path, 'w') as fout2:\n",
    "        readin = f.readlines()\n",
    "        for line in tqdm(readin):\n",
    "            tmp = json.loads(line)\n",
    "            fout1.write(str(docid) + '\\t' + tmp['q_text'] + '\\n')\n",
    "            for label in tmp['labels']:\n",
    "                fout2.write(str(docid)+' '+str(0)+' '+str(label)+' '+str(1)+'\\n')\n",
    "            docid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_name2id_dict = init_retrieval_label(data)\n",
    "write_retrieval_base(save_base_dir, data_name, 'retrieval', data, label_name2id_dict)\n",
    "write_retrieval(save_base_dir, data_name, 'retrieval', label_name2id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "684dd79593b7a31fd1a3d9b9f79e274b2833c74c2798fa00dbf737909b6a5be6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
