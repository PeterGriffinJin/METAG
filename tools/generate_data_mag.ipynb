{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code script is to sample and generate data for representation learning on MAG networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import itertools\n",
    "import functools\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Set, Tuple\n",
    "import numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_root:str, dataset:str, sub_dataset:str) -> Dict:\n",
    "    \"\"\"\n",
    "    data_root: path to directory contains the data file.\n",
    "    dataset: path to dataset (MAG/Amazon)\n",
    "    subdataset: sub dataset name (e.g. CS, sports)\n",
    "\n",
    "    Returns:\n",
    "    data: Dict, key is the doc id, and value is data entry\n",
    "    \"\"\"\n",
    "    # read raw data\n",
    "    data_path = os.path.join(data_root, dataset, sub_dataset, 'papers_bert.json')\n",
    "    with open(data_path) as f:\n",
    "        data = {}\n",
    "        readin = f.readlines()\n",
    "        for line in tqdm(readin, desc=\"Loading Data...\"):\n",
    "            tmp = eval(line.strip())\n",
    "            k = tmp['paper']\n",
    "            data[k] = tmp\n",
    "            data[k]['citation'] = []\n",
    "    for k in data:\n",
    "        refs = data[k]['reference']\n",
    "        new_refs = []\n",
    "        for paper in refs:\n",
    "            if paper in data:\n",
    "                new_refs.append(paper)\n",
    "                data[paper]['citation'].append(k)\n",
    "        data[k]['reference'] = new_refs\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_no_intermediate(data: Dict, type: List[str], max_sample:int = 1250000) -> Set[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    data: dataset return by `load_data`\n",
    "    type: list of length 1\n",
    "\n",
    "    Returns:\n",
    "    id_pair: set of id pairs sampled\n",
    "    \"\"\"\n",
    "    t = type[0]\n",
    "    id_pair = set()\n",
    "    keys = list(data.keys())\n",
    "    cnt = np.zeros(len(data))\n",
    "    for i, k0 in tqdm(enumerate(keys)):\n",
    "        cnt[i] = len(data[k0][t])\n",
    "    ss = cnt.sum()\n",
    "    prob = cnt / ss\n",
    "    for idx, k0 in enumerate(tqdm(keys)):\n",
    "        tmp = prob[idx] * max_sample\n",
    "        num_to_sample = int(tmp)\n",
    "        fl = tmp - num_to_sample\n",
    "        if np.random.uniform(0, 1) <= fl:\n",
    "            num_to_sample += 1\n",
    "        if num_to_sample > 0:\n",
    "            lst = data[k0][t]\n",
    "            random.shuffle(lst)\n",
    "            tmpcnt = 0\n",
    "            for k1 in lst:\n",
    "                if k0 != k1 and (k0, k1) not in id_pair:\n",
    "                    id_pair.add((k0, k1))\n",
    "                    tmpcnt += 1\n",
    "                    if tmpcnt >= num_to_sample:\n",
    "                        break\n",
    "    print(len(id_pair))\n",
    "    return id_pair "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_one_intermediate(data:Dict, type: List[str],  max_sample=1250000) -> Set[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    data: dataset return by `load_data`\n",
    "    type: list of length 1\n",
    "\n",
    "    Returns:\n",
    "    id_pair: set of id pairs sampled\n",
    "    \"\"\"\n",
    "    @functools.lru_cache\n",
    "    def idx2coord(idx):\n",
    "        xx = math.ceil(math.sqrt(2*idx+0.25)-0.5)\n",
    "        yy = idx - xx * (xx-1) // 2\n",
    "        return xx-1, yy-1\n",
    "\n",
    "    def sample_random_pair_in_list(lst: List, number:int, res_set: Set):\n",
    "        n = len(lst)\n",
    "        pair_cnt = n*(n+1) // 2\n",
    "        to_sample = np.random.permutation(pair_cnt)\n",
    "        cnt = 0\n",
    "        for i in range(len(to_sample)):\n",
    "            idx = to_sample[i]+1\n",
    "            xx, yy = idx2coord(idx)\n",
    "            assert yy <= xx\n",
    "            cur_pair = (lst[xx], lst[yy])\n",
    "            if cur_pair not in res_set:\n",
    "                res_set.add(cur_pair)\n",
    "                cnt += 1\n",
    "            if cnt == number:\n",
    "                break\n",
    "        return cnt\n",
    "\n",
    "    t = type[0]\n",
    "    id_pair = set()\n",
    "    co_type = defaultdict(set)\n",
    "    for k0 in tqdm(data):\n",
    "        inter = data[k0][t]\n",
    "        if isinstance(inter, list) or isinstance(inter, set):\n",
    "            for x in inter:\n",
    "                co_type[x].add(k0)\n",
    "        else:\n",
    "            co_type[inter].add(k0)\n",
    "    keys = list(co_type.keys())\n",
    "    cnt = np.zeros(len(keys))\n",
    "    for i, k in enumerate(keys):\n",
    "        cnt[i] = len(co_type[k])\n",
    "    cnt = cnt * (cnt+1) / 2.0\n",
    "    ss = cnt.sum()\n",
    "    prob = cnt / ss \n",
    "    for idx, k in enumerate(tqdm(keys)):\n",
    "        num_sample = int(prob[idx] * max_sample)\n",
    "        deci = prob[idx] * max_sample - num_sample\n",
    "        if np.random.uniform(0, 1) <= deci:\n",
    "            num_sample += 1\n",
    "        if num_sample >= 1:\n",
    "            lst = list(co_type[k])\n",
    "            true_sample = sample_random_pair_in_list(lst, num_sample, id_pair)\n",
    "            # print(true_sample, num_sample)\n",
    "    print(len(id_pair))\n",
    "    return id_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_dump(data: Dict, tuples: Set[Tuple[str, str]], path: str) -> None:\n",
    "    \"\"\"\n",
    "    Dump the sampled pairs into jsonl file\n",
    "\n",
    "    data: Dataset returned by `load_data`\n",
    "    tuples: Sampled tuples\n",
    "    path: path to save json file\n",
    "    \"\"\"\n",
    "    print(\"Dump data to %s\" % path)\n",
    "    with open(path, 'w') as fout:\n",
    "        for t in tqdm(tuples, desc=\"Processing %s\" % path.split('/')[-1]):\n",
    "            q, k = t\n",
    "            cur = {}\n",
    "            cur['q_text'] = data[q]['title']\n",
    "            cur['k_text'] = data[k]['title']\n",
    "            fout.write(json.dumps(cur)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_DICT = {\n",
    "    'pr': build_no_intermediate,\n",
    "    'pc': build_no_intermediate,\n",
    "    'pap': build_one_intermediate,\n",
    "    'pvp': build_one_intermediate,\n",
    "    'pcp': build_one_intermediate,\n",
    "    'prp': build_one_intermediate\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['MAG'][0]\n",
    "sub_datasets = ['Mathematics'][0]\n",
    "base_dir = 'xxx/data/'\n",
    "save_dir = f'xxx/data/{sub_datasets}/raw'\n",
    "\n",
    "cur_d = load_data(base_dir, datasets, sub_datasets)\n",
    "print(len(cur_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in cur_d:\n",
    "    print(k)\n",
    "    print(cur_d[k])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = GENERATOR_DICT['pr'](cur_d, ['reference'])\n",
    "convert_and_dump(cur_d, pr, os.path.join(save_dir, 'pp.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pc = GENERATOR_DICT['pc'](cur_d, ['citation'])\n",
    "convert_and_dump(cur_d, pc, os.path.join(save_dir, 'pc.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pap = GENERATOR_DICT['pap'](cur_d, ['author'])\n",
    "convert_and_dump(cur_d, pap, os.path.join(save_dir, 'pap.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pvp = GENERATOR_DICT['pvp'](cur_d, ['venue'])\n",
    "convert_and_dump(cur_d, pvp, os.path.join(save_dir, 'pvp.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prp = GENERATOR_DICT['prp'](cur_d, ['reference'])\n",
    "convert_and_dump(cur_d, prp, os.path.join(save_dir, 'prp.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp = GENERATOR_DICT['pcp'](cur_d, ['citation'])\n",
    "convert_and_dump(cur_d, pcp, os.path.join(save_dir, 'pcp.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "684dd79593b7a31fd1a3d9b9f79e274b2833c74c2798fa00dbf737909b6a5be6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
